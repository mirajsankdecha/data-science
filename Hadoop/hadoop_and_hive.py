# -*- coding: utf-8 -*-
"""Hadoop_and_Hive.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1T1DvQixYkNrXgRDWGH-T5whvwY1Xd67W
"""



"""#Hive with Hadoop
This notebook has all the codes / commands required to install Hadoop and Hive <br>

#1 Hadoop
Hadoop is a pre-requisite for Hive <br>

## 1.1 Download, Install Hadoop
"""

# The default JVM available at /usr/lib/jvm/java-11-openjdk-amd64/  works for Hadoop
# But gives errors with Hive https://stackoverflow.com/questions/54037773/hive-exception-class-jdk-internal-loader-classloadersappclassloader-cannot
# Hence this JVM needs to be installed
!apt-get update > /dev/null
!apt-get install openjdk-8-jdk-headless -qq > /dev/null


# If there is an error in this cell, it is very likely that the version of hadoop has changed
# Download the latest version of Hadoop and change the version numbers accordingly
#wget -q https://downloads.apache.org/hadoop/common/hadoop-3.3.0/hadoop-3.3.0.tar.gz
#!wget -q https://downloads.apache.org/hadoop/common/hadoop-3.3.2/hadoop-3.3.2.tar.gz
#!wget  https://downloads.apache.org/hadoop/common/hadoop-3.3.2/hadoop-3.3.2.tar.gz
!wget -q https://downloads.apache.org/hadoop/common/hadoop-3.3.5/hadoop-3.3.5.tar.gz
# Unzip it
# the tar command with the -x flag to extract, -z to uncompress, -v for verbose output, and -f to specify that weâ€™re extracting from a file
#!tar -xzf hadoop-3.3.2.tar.gz
!tar -xzf hadoop-3.3.5.tar.gz
#copy  hadoop file to user/local
#!mv  hadoop-3.3.2/ /usr/local/
!mv  hadoop-3.3.5/ /usr/local/

!ls

"""## 1.2 Set Environment Variables

"""

#To set java path, go to /usr/local/hadoop-3.3.0/etc/hadoop/hadoop-env.sh then
#. . . export JAVA_HOME=/usr/lib/jvm/java-11-openjdk-amd64/ . . .
#we have used a simpler alternative route using os.environ - it works

import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"   # default is changed
#os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-11-openjdk-amd64/"
# make sure that the version number is as downloaded
#os.environ["HADOOP_HOME"] = "/usr/local/hadoop-3.3.0/"
#os.environ["HADOOP_HOME"] = "/usr/local/hadoop-3.3.2/"
os.environ["HADOOP_HOME"] = "/usr/local/hadoop-3.3.5/"

# Add Hadoop BIN to PATH
# Get the current_path from output of previous command
#current_path = '/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin:/opt/bin'
#current_path = '/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin:/opt/bin'
#new_path = current_path+':/usr/local/hadoop-3.3.2/bin/'
#os.environ["PATH"] = new_path
#!echo $PATH

current_path = os.getenv('PATH')
#new_path = current_path+':/usr/local/hadoop-3.3.0/bin/'
#new_path = current_path+':/usr/local/hadoop-3.3.2/bin/'
new_path = current_path+':/usr/local/hadoop-3.3.5/bin/'
os.environ["PATH"] = new_path
!echo $PATH

"""## 1.3 Test Hadoop Installation"""

#Running Hadoop - Test RUN, not doing anything at all
#!/usr/local/hadoop-3.3.0/bin/hadoop
# UNCOMMENT the following line if you want to make sure that Hadoop is alive!
#!hadoop

# Testing Hadoop with PI generating sample program, should calculate value of pi = 3.14157500000000000000
# pi example
#Uncomment the following line if  you want to test Hadoop with pi example
#!hadoop jar /usr/local/hadoop-3.3.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.0.jar pi 16 100000

"""#2 Hive

## 2.1 Download, Install HIVE
"""

# Download and Unzip the correct version and unzip
#!wget https://downloads.apache.org/hive/hive-3.1.2/apache-hive-3.1.2-bin.tar.gz
#!wget -q https://downloads.apache.org/hive/hive-3.1.3/apache-hive-3.1.3-bin.tar.gz
!wget -c https://archive.apache.org/dist/hive/hive-3.1.3/apache-hive-3.1.3-bin.tar.gz -O hive.tar.gz

#!tar xzf apache-hive-3.1.2-bin.tar.gz
#!tar xzf apache-hive-3.1.3-bin.tar.gz

!tar -xzf hive.tar.gz

"""## 2.2 Set Environment *Variables*"""

# Make sure that the version number is correct and is as downloaded
#os.environ["HIVE_HOME"] = "/content/apache-hive-3.1.2-bin"
os.environ["HIVE_HOME"] = "/content/apache-hive-3.1.3-bin"
!echo $HIVE_HOME

# current_path is set from output of previous command
#current_path = '/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin:/opt/bin:/usr/local/hadoop-3.3.0/bin/'
#current_path = '/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin:/usr/local/hadoop-3.3.2/bin/'
#new_path = current_path+':/content/apache-hive-3.1.2-bin/bin'
#os.environ["PATH"] = new_path
#!echo $PATH


current_path = os.getenv('PATH')
#new_path = current_path+':/usr/local/hadoop-3.3.0/bin/'
#new_path = current_path+':/content/apache-hive-3.1.2-bin/bin'
new_path = current_path+':/content/apache-hive-3.1.3-bin/bin'
os.environ["PATH"] = new_path
!echo $PATH

!echo $JAVA_HOME
!echo $HADOOP_HOME
!echo $HIVE_HOME

"""## 2.3 Set up HDFS Directories"""

!hdfs dfs -mkdir /tmp
!hdfs dfs -chmod g+w /tmp
#!hdfs dfs -ls /
!hdfs dfs -mkdir -p /content/warehouse
!hdfs dfs -chmod g+w /content/warehouse
#!hdfs dfs -ls /content/

"""## 2.4 Initialise HIVE - note and fix errors"""

# TYPE this command, do not copy and paste. Non printing characters cause havoc
# There will be two errors, that we will fix
# UNCOMMENT the following line if you WISH TO SEE the errors
#!schematool -initSchema -dbType derby

"""### 2.4.1 Fix One Warning, One Error
SLF4J is duplicate, need to locate them and remove one <br>
Guava jar version is low
"""

# locate multiple instances of slf4j ...
!ls $HADOOP_HOME/share/hadoop/common/lib/*slf4j*
!ls $HIVE_HOME/lib/*slf4j*

# removed the logging jar from Hive, retaining the Hadoop jar
#!mv /content/apache-hive-3.1.2-bin/lib/log4j-slf4j-impl-2.10.0.jar ./
!mv /content/apache-hive-3.1.3-bin/lib/log4j-slf4j-impl-2.17.1.jar ./

# guava jar needs to above v 20
# https://stackoverflow.com/questions/45247193/nosuchmethoderror-com-google-common-base-preconditions-checkargumentzljava-lan
!ls $HIVE_HOME/lib/gu*

# the one available with Hadoop is better, v 27
!ls $HADOOP_HOME/share/hadoop/hdfs/lib/gu*

# Remove the Hive Guava and replace with Hadoop Guava
!mv $HIVE_HOME/lib/guava-19.0.jar ./

!cp $HADOOP_HOME/share/hadoop/hdfs/lib/guava-27.0-jre.jar $HIVE_HOME/lib/

"""# **Initialize HIVE**"""

# Commented out IPython magic to ensure Python compatibility.
!pip -q install colab-xterm
# %load_ext colabxterm

# Commented out IPython magic to ensure Python compatibility.
# %xterm

!schematool -initSchema -dbType derby

!hive -e"\
create database if not exists praxisDB;\
show databases;\
"

!hive -database praxisdb -e "\
create table if not exists emp2 (name string, age int);\
show tables;\
"

!hive -database praxisdb -e "\
insert into emp2 values ('nilesh',27);\
insert into emp2 values ('nayan',26);\
"

!hive -database praxisdb -e "insert into emp2 values ('nilesh',27); insert into emp2 values ('nayan',26);"

!hive -database praxisdb -e "\
select * from emp2;\
select * from emp2 where name = 'nayan';\
"

!hive -e"\
create database if not exists nbs;\
show databases;\
"

!hive -database nbs -e "\
create table if not exists students (name string, roll_no int, semester int, total_grade string);\
show tables;\
"

!hive -database nbs -e "\
create table if not exists faculties (name string, department string, performance int);\
show tables;\
"

!hive -database nbs -e "\
insert into students values ('nilesh',01,3,'A');\
insert into students values ('nayan',01,3,'F');\
insert into students values ('miraj',03,3,'D');\
insert into students values ('sneha',04,3,'B');\
insert into students values ('sinu',05,3,"A");\
"

!hive -database nbs -e "\
insert into faculties values ('yogi','teacher',5);\
insert into faculties values ('pratima','teacher',2);\
insert into faculties values ('neha','admin',5);\
insert into faculties values ('sikha','principle',5);\
insert into faculties values ('pihu','lab_assistant',5);\
"

!hive -database nbs -e "SELECT * FROM students;"
!hive -database nbs -e "SELECT * FROM faculties;"

!hive -database nbs -e "SELECT * FROM students WHERE total_grade = 'A';"

!hive -database nbs -e "SELECT name, performance FROM faculties;"

from google.colab import files
uploaded = files.upload()

!head SS_Orders.csv

!hive -S -database praxisDB -e "\
drop table if exists ss_order;\
CREATE TABLE IF NOT EXISTS ss_order (\
    RowID smallint,\
    OrderID char(14), OrderDate string,\
    ShipDate string, ShipMode varchar(16),\
    CustomerID char(8), CustomerName varchar(30), Segment varchar(20),\
    Country varchar(30), City varchar(30), State varchar(30), PostalCode char(5), Region varchar(15),\
    ProductID varchar(20), Category varchar(40), SubCategory varchar(40), ProductName varchar(200),\
    Sales decimal(8,2), Quantity smallint, Discount decimal(4,2), Profit decimal(8,2)\
) row format delimited fields terminated by ',';\
describe ss_order;\
"

!sed 's/\r//' /content/SS_Orders.csv > datafile.csv

!sed -i -e "1d" datafile.csv

!head datafile.csv

!hive -S -database praxisdb -e "\
TRUNCATE TABLE ss_order;\
LOAD DATA LOCAL INPATH 'datafile.csv' INTO TABLE ss_order;\
"

#retrieve all the rows and columns from the dataset
!hive -S -database praxisdb -e "\
select count(*) from ss_order;\
select * from ss_order limit 10;\
"

#find out the total sales of the company
!hive -S -database praxisdb -e "\
select sum(sales) from ss_order;\
"

# find out the total sales category wise
!hive -S -database praxisdb -e "\
select category, sum(sales) from ss_order group by category;\
"

# find out the 1st date and last date of sale in ascending order
!hive -S -database praxisdb -e "\
select min(orderdate), max(orderdate) from ss_order;\
"

!hive -S -database -e "\
SELECT SUBSTR(OrderDate, 4, 2) AS month, sum(Sales) AS monthly_sales FROM ss_order\
GROUP BY SUBSTR(OrderDate, 4, 2);\

!hive -S -database praxisdb -e "\
SELECT SUBSTR(OrderDate, 7, 4) AS year, sum(Sales) AS yearly_sales FROM ss_order\
GROUP BY SUBSTR(OrderDate, 7, 4);\
"

!hive -S -database praxisdb -e "\
SELECT OrderDate, ShipDate FROM ss_order\
WHERE TO_DATE(ShipDate) IS NULL OR TO_DATE(OrderDate) IS NULL\
LIMIT 10;\

!hive -S -database praxisdb -e "\
SELECT AVG(DATEDIFF(TO_DATE(FROM_UNIXTIME(UNIX_TIMESTAMP(ShipDate,'dd/MM/yyyy'))),\
                    TO_DATE(FROM_UNIXTIME(UNIX_TIMESTAMP(OrderDate,'dd/MM/yyyy'))))) AS avg_days_to_ship FROM ss_order\