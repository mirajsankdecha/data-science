{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e5a9cc4-6646-478d-9854-df78a9bfac99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe0f5230-bf94-4c54-9bd2-bb16bdda1775",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Apache Spark RDD Operations with Detailed Explanation\n",
    "\n",
    "This notebook demonstrates various RDD (Resilient Distributed Dataset) operations in Apache Spark using Databricks. It includes explanations of how each operation works and its significance in distributed data processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32be7406-70ef-41e0-80ab-1804e67d8097",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Initializing SparkContext\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea30ed76-6f18-4af8-b3fe-a7b9ae4e4708",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Creating an RDD\n",
    "**What is happening?**\n",
    "- We use `parallelize()` to create an RDD from a Python list.\n",
    "- RDD (Resilient Distributed Dataset) is a fundamental data structure in Spark, representing an immutable distributed collection of objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7678518-ffa6-4369-b349-9c731e4d0742",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD Contents: [1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "num = [1, 2, 3, 4, 5]\n",
    "num_rdd = sc.parallelize(num)\n",
    "print(\"RDD Contents:\", num_rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0807b165-3cca-482b-b806-105ad7acb67c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Finding Maximum Value\n",
    "**What is happening?**\n",
    "- We use the `reduce()` function to find the maximum value in the RDD.\n",
    "- `reduce()` works by applying a binary function cumulatively to the elements of the RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5476579-fb15-4a5c-bd7e-6ebaac2e9dc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Value: 15\n"
     ]
    }
   ],
   "source": [
    "num_list = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15]\n",
    "num_rdd = sc.parallelize(num_list)\n",
    "max_value = num_rdd.reduce(lambda x, y: x if x > y else y)\n",
    "print(\"Maximum Value:\", max_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59738bdc-6666-44d2-a587-a82248992b49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Applying `map()` Transformation\n",
    "**What is happening?**\n",
    "- `map()` is a transformation that applies a given function to each element of the RDD.\n",
    "- Here, we multiply each element by 2.\n",
    "- The output is a new RDD containing the transformed values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15fcc4cb-3d69-4083-9307-1dcd4e646e3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doubled Values: [2, 4, 6, 8, 10]\n"
     ]
    }
   ],
   "source": [
    "n_rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "final_res = n_rdd.map(lambda y: y * 2).collect()\n",
    "print(\"Doubled Values:\", final_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb19ddbb-2948-40cd-ab32-6f3b92f749d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Filtering Even Numbers\n",
    "**What is happening?**\n",
    "- `filter()` is used to keep only elements that satisfy a given condition.\n",
    "- We filter out even numbers by checking if each element is divisible by 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d4eec7f-6143-4b68-bb4b-f3c3ae7fb75c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Even Numbers: [2, 4, 6, 8, 10, 12, 14, 16, 18, 20]\n"
     ]
    }
   ],
   "source": [
    "rdd = sc.parallelize(range(1, 21))  # RDD from 1 to 20\n",
    "final_res = rdd.filter(lambda x: x % 2 == 0).collect()\n",
    "print(\"Even Numbers:\", final_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "110da5af-fbcc-4069-8b58-1eeac5673d7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Sorting RDD Based on Elements\n",
    "**What is happening?**\n",
    "- We use `sortBy()` to sort elements based on a specific index in a tuple.\n",
    "- Sorting can be done in ascending or descending order.\n",
    "- Here, we sort by the third and second values in the tuple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0a8fd2b-7ab1-44fa-bd23-950402cde45d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted by Third Element Ascending: [('d', 3, 9), ('b', 3, 9), ('a', 5, 10), ('c', 2, 11)]\nSorted by Third Element Descending: [('c', 2, 11), ('a', 5, 10), ('d', 3, 9), ('b', 3, 9)]\nSorted by Second Element Ascending: [('c', 2, 11), ('d', 3, 9), ('b', 3, 9), ('a', 5, 10)]\n"
     ]
    }
   ],
   "source": [
    "pairs = [(\"a\", 5, 10), (\"d\", 3, 9), (\"c\", 2, 11), (\"b\", 3, 9)]\n",
    "rdd = sc.parallelize(pairs)\n",
    "\n",
    "sorted_by_third_asc = rdd.sortBy(lambda x: x[2])\n",
    "print(\"Sorted by Third Element Ascending:\", sorted_by_third_asc.collect())\n",
    "\n",
    "sorted_by_third_desc = rdd.sortBy(lambda x: x[2], ascending=False)\n",
    "print(\"Sorted by Third Element Descending:\", sorted_by_third_desc.collect())\n",
    "\n",
    "sorted_by_second_asc = rdd.sortBy(lambda x: x[1])\n",
    "print(\"Sorted by Second Element Ascending:\", sorted_by_second_asc.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "730244a2-3f8e-4fe3-8ff6-46c0f81e42f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Sorting RDD Using `sortByKey()`\n",
    "**What is happening?**\n",
    "- `sortByKey()` sorts an RDD based on the key in (key, value) pairs.\n",
    "- Sorting can be in ascending or descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ef64ce7-17d0-4d15-b08b-9d1c0768babd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted by Key Ascending: [(1, 'banana'), (2, 'cherry'), (3, 'apple'), (4, 'date')]\nSorted by Key Descending: [(4, 'date'), (3, 'apple'), (2, 'cherry'), (1, 'banana')]\n"
     ]
    }
   ],
   "source": [
    "key_value_pairs = sc.parallelize([(3, \"apple\"), (1, \"banana\"), (2, \"cherry\"), (4, \"date\")])\n",
    "\n",
    "sorted_asc = key_value_pairs.sortByKey()\n",
    "print(\"Sorted by Key Ascending:\", sorted_asc.collect())\n",
    "\n",
    "sorted_desc = key_value_pairs.sortByKey(ascending=False)\n",
    "print(\"Sorted by Key Descending:\", sorted_desc.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c5b92b8-d123-4784-aa7e-631a7b039719",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Using a Function with `map()`\n",
    "**What is happening?**\n",
    "- Instead of using a lambda function, we define a function `eve_function()` that doubles a value.\n",
    "- We pass this function to `map()` to transform the RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b1b5fec-3a97-45aa-bb37-3a2f0a23cd73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function-based Mapping Result: [2, 4, 6, 8, 10]\n"
     ]
    }
   ],
   "source": [
    "def eve_function(x):\n",
    "    return x * 2\n",
    "\n",
    "n_rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "final_res = n_rdd.map(eve_function).collect()\n",
    "print(\"Function-based Mapping Result:\", final_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "608ffdc9-037f-4c98-b279-6bb50cfd1093",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Filtering Even Numbers with `filter()`\n",
    "**What is happening?**\n",
    "- We use `filter()` to keep only even numbers from the RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0a74ff1-54a2-42ff-9b10-08c94f8d7813",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered Even Numbers: [2, 4]\n"
     ]
    }
   ],
   "source": [
    "listt = [1, 2, 3, 4, 5]\n",
    "rdd = sc.parallelize(listt)\n",
    "even_rdd = rdd.filter(lambda x: x % 2 == 0)\n",
    "print(\"Filtered Even Numbers:\", even_rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73375c8e-1138-4fcb-b068-8d00c4634c3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## `flatMap()` Transformation\n",
    "**What is happening?**\n",
    "- Unlike `map()`, `flatMap()` flattens the output.\n",
    "- Each element produces multiple values, and they are returned as a single list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e06a6c13-0623-47e0-8136-4cd82a3a9c76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened Values: [1, 1, 2, 1, 2, 3, 1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "num_rdd = sc.parallelize([1, 2, 3, 4, 5])\n",
    "final_res = num_rdd.flatMap(lambda x: range(1, x)).collect()\n",
    "print(\"Flattened Values:\", final_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4327f392-a8ec-409f-9fc6-8b59facc8890",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Removing Duplicates with `distinct()`\n",
    "**What is happening?**\n",
    "- `distinct()` removes duplicate elements from an RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "991f9f5a-392f-4bbe-8ecf-fce3a912c500",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distinct Elements: [1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "num_rdd = sc.parallelize([1, 2, 3, 4, 5, 1, 2, 3, 4, 5])\n",
    "final_res = num_rdd.distinct().collect()\n",
    "print(\"Distinct Elements:\", final_res)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Spark Installation",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
