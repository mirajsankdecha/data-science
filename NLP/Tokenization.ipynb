{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b0df99a-28fc-4c93-ac2d-0c18b0209f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.9.2)\n",
      "Requirement already satisfied: click in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (8.2.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (2025.11.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (4.67.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (4.54.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.4.7)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (1.26.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c10ba95-ab6a-4813-87c2-526c48fc1310",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8c4ce14d-42cd-44fb-a4e8-2700cbdd1721",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0fd46bbf-c43f-478e-a95e-e6f2ce71e1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Tokenization:\n",
      "['Hello', ',', 'world', '!', 'This', 'is', 'an', 'NLP', 'exercise', '.', 'Let', \"'s\", 'learn', 'tokenization', '.']\n",
      "\n",
      "Sentence Tokenization:\n",
      "['Hello, world!', 'This is an NLP exercise.', \"Let's learn tokenization.\"]\n",
      "\n",
      "Character Tokenization:\n",
      "['H', 'e', 'l', 'l', 'o', ',', ' ', 'w', 'o', 'r', 'l', 'd', '!', ' ', 'T', 'h', 'i', 's', ' ', 'i', 's', ' ', 'a', 'n', ' ', 'N', 'L', 'P', ' ', 'e', 'x', 'e', 'r', 'c', 'i', 's', 'e', '.', ' ', 'L', 'e', 't', \"'\", 's', ' ', 'l', 'e', 'a', 'r', 'n', ' ', 't', 'o', 'k', 'e', 'n', 'i', 'z', 'a', 't', 'i', 'o', 'n', '.']\n",
      "\n",
      "Regex Tokenization:\n",
      "['Hello', 'world', 'This', 'is', 'an', 'NLP', 'exercise', 'Let', 's', 'learn', 'tokenization']\n"
     ]
    }
   ],
   "source": [
    "def tokenization_exercise():\n",
    "    \"\"\"Demonstrate different types of tokenization.\"\"\"\n",
    "    \n",
    "    # Sample text\n",
    "    text = \"Hello, world! This is an NLP exercise. Let's learn tokenization.\"\n",
    "    \n",
    "    # Word Tokenization\n",
    "    word_tokens = nltk.word_tokenize(text)\n",
    "    print(\"Word Tokenization:\")\n",
    "    print(word_tokens)\n",
    "\n",
    "    # Sentence Tokenization\n",
    "    sentence_tokens = nltk.sent_tokenize(text)\n",
    "    print(\"\\nSentence Tokenization:\")\n",
    "    print(sentence_tokens)\n",
    "\n",
    "    # Character Tokenization\n",
    "    char_tokens = list(text)\n",
    "    print(\"\\nCharacter Tokenization:\")\n",
    "    print(char_tokens)\n",
    "\n",
    "    # Regex Tokenization\n",
    "    import re\n",
    "    regex_tokens = re.findall(r'\\w+', text)\n",
    "    print(\"\\nRegex Tokenization:\")\n",
    "    print(regex_tokens)\n",
    "\n",
    "# Run the tokenization exercise\n",
    "tokenization_exercise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d65bd968-34af-40eb-bf9f-12f92967a847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hi  Hello world  Today is          '"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def pre_process(text):\n",
    "    return re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "\n",
    "pre_process(\"Hi, Hello world! Today is 19 ** &&&\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2ec80379-5095-454d-8736-3a5a78cb08cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n",
      ",\n",
      "world\n",
      "!\n",
      "NLP\n",
      "exercise\n",
      ".\n",
      "Let\n",
      "'s\n",
      "learn\n",
      "tokenization\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Load stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "text = \"Hello, world! This is an NLP exercise. Let's learn tokenization.\"\n",
    "\n",
    "# Word tokenization\n",
    "word_tokens = nltk.word_tokenize(text)\n",
    "\n",
    "# Filtering words\n",
    "for word in word_tokens:\n",
    "    if word.lower() not in stop_words:\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f8fcf9a-ce9c-4d47-bba5-01618040a454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tokens:\n",
      "['this', 'is', 'a', 'sample', 'sentence', 'demonstrating', 'stopwords', 'removal', 'in', 'nlp', '.']\n",
      "\n",
      "Tokens after Stopwords Removal:\n",
      "['sample', 'sentence', 'demonstrating', 'stopwords', 'removal', 'nlp', '.']\n"
     ]
    }
   ],
   "source": [
    "# Sample text\n",
    "text = \"This is a sample sentence demonstrating stopwords removal in NLP.\"\n",
    "\n",
    "# Get English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Tokenize the text\n",
    "word_tokens = nltk.word_tokenize(text.lower())\n",
    "\n",
    "# Remove stopwords\n",
    "filtered_tokens = [word for word in word_tokens if word not in stop_words]\n",
    "\n",
    "print(\"Original Tokens:\")\n",
    "print(word_tokens)\n",
    "\n",
    "print(\"\\nTokens after Stopwords Removal:\")\n",
    "print(filtered_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd477bb-2da9-44e3-83f5-111f4f455a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOKENIZER → breaks text into words\n",
    "# STEMMER → cuts words roughly (not always meaningful)\n",
    "# LEMMATIZER → finds the real dictionary root word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d711e85c-6d70-4eb8-b219-6a951d8e321c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'PorterStemmer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 24\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mLemmatized Words:\u001b[39m\u001b[38;5;124m\"\u001b[39m, lemmatized_words)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Run the function\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m \u001b[43mstemming_lemmatization_exercise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 11\u001b[0m, in \u001b[0;36mstemming_lemmatization_exercise\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m words \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrunning\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mflies\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetter\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwas\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnetworking\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgoes\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkites\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Porter Stemmer\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m porter_stemmer \u001b[38;5;241m=\u001b[39m \u001b[43mPorterStemmer\u001b[49m()\n\u001b[0;32m     12\u001b[0m stemmed_words \u001b[38;5;241m=\u001b[39m [porter_stemmer\u001b[38;5;241m.\u001b[39mstem(word) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words]\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# WordNet Lemmatizer\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'PorterStemmer' is not defined"
     ]
    }
   ],
   "source": [
    "def stemming_lemmatization_exercise():\n",
    "    \"\"\"\n",
    "    Compare stemming and lemmatization techniques\n",
    "    \"\"\"\n",
    "\n",
    "    # Download required data (only once)\n",
    "    nltk.download('wordnet')\n",
    "    nltk.download('omw-1.4')\n",
    "\n",
    "    # Sample words\n",
    "    words = ['running', 'flies', 'better', 'is', 'was', 'networking', 'goes', 'kites']\n",
    "\n",
    "    # Porter Stemmer\n",
    "    porter_stemmer = PorterStemmer()\n",
    "    stemmed_words = [porter_stemmer.stem(word) for word in words]\n",
    "\n",
    "    # WordNet Lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "    print(\"Original Words:\", words)\n",
    "    print(\"\\nStemmed Words:\", stemmed_words)\n",
    "    print(\"\\nLemmatized Words:\", lemmatized_words)\n",
    "\n",
    "\n",
    "# Run the function\n",
    "stemming_lemmatization_exercise()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa9df79-3452-424c-898e-587f6d9ebb7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
